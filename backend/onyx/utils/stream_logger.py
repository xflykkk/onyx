"""
æµå¼å“åº”å†…å®¹ç›‘å¬å™¨
ç”¨äºè°ƒè¯•å’Œåˆ†æ SSE æµå¼æ•°æ®
"""
import json
import os
import uuid
from datetime import datetime
from typing import Dict, Any, List
from collections.abc import Generator

from onyx.configs.app_configs import ENABLE_STREAM_LOGGING, STREAM_LOG_DIR


class StreamLogger:
    """æµå¼å†…å®¹ç›‘å¬å™¨ï¼Œè®°å½•æ‰€æœ‰ SSE æ•°æ®åŒ…åˆ°æ–‡ä»¶"""
    
    def __init__(self, request_info: Dict[str, Any] = None):
        self.request_id = str(uuid.uuid4())[:8]
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.request_info = request_info or {}
        
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        self.log_dir = STREAM_LOG_DIR
        os.makedirs(self.log_dir, exist_ok=True)
        
        # ç”Ÿæˆæ—¥å¿—æ–‡ä»¶è·¯å¾„
        self.log_file = os.path.join(
            self.log_dir, 
            f"stream_{self.timestamp}_{self.request_id}.json"
        )
        
        # å­˜å‚¨æ‰€æœ‰æ•°æ®åŒ…
        self.packets: List[Dict[str, Any]] = []
        self.packet_count = 0
        
        # åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶
        self._init_log_file()
    
    def _init_log_file(self):
        """åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶ï¼Œå†™å…¥å…ƒæ•°æ®"""
        metadata = {
            "request_id": self.request_id,
            "timestamp": self.timestamp,
            "start_time": datetime.now().isoformat(),
            "request_info": self._serialize_request_info(self.request_info),
            "packets": []
        }
        
        with open(self.log_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“ Stream log initialized: {self.log_file}")
    
    def _serialize_request_info(self, request_info: Dict[str, Any]) -> Dict[str, Any]:
        """åºåˆ—åŒ–è¯·æ±‚ä¿¡æ¯ï¼Œå¤„ç†ä¸èƒ½ JSON åºåˆ—åŒ–çš„å¯¹è±¡"""
        serialized = {}
        for key, value in request_info.items():
            try:
                # å°è¯•åºåˆ—åŒ–å€¼
                json.dumps(value)
                serialized[key] = value
            except TypeError:
                # å¦‚æœä¸èƒ½åºåˆ—åŒ–ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
                serialized[key] = str(value)
        return serialized
    
    def log_packet(self, packet_data: str, packet_type: str = "sse"):
        """è®°å½•å•ä¸ªæ•°æ®åŒ…"""
        self.packet_count += 1
        
        # è§£æ JSON æ•°æ®åŒ…
        try:
            parsed_data = json.loads(packet_data) if packet_data.strip() else {}
        except json.JSONDecodeError:
            parsed_data = {"raw_content": packet_data}
        
        packet_entry = {
            "sequence": self.packet_count,
            "timestamp": datetime.now().isoformat(),
            "type": packet_type,
            "raw_data": packet_data,
            "parsed_data": parsed_data,
            "data_size": len(packet_data)
        }
        
        self.packets.append(packet_entry)
        
        # å®æ—¶å†™å…¥æ–‡ä»¶ï¼ˆè¿½åŠ æ¨¡å¼ï¼‰
        self._append_packet_to_file(packet_entry)
        
        # æ§åˆ¶å°è¾“å‡ºï¼ˆä»…åœ¨å¯ç”¨æµæ—¥å¿—æ—¶ï¼‰
        if ENABLE_STREAM_LOGGING:
            print(f"ğŸ“¦ #{self.packet_count} {packet_type}: {packet_data[:100]}...")
    
    def _append_packet_to_file(self, packet_entry: Dict[str, Any]):
        """è¿½åŠ æ•°æ®åŒ…åˆ°æ–‡ä»¶"""
        try:
            # è¯»å–ç°æœ‰å†…å®¹
            with open(self.log_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # æ·»åŠ æ–°æ•°æ®åŒ…
            data["packets"].append(packet_entry)
            data["packet_count"] = self.packet_count
            data["last_update"] = datetime.now().isoformat()
            
            # å†™å›æ–‡ä»¶
            with open(self.log_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            print(f"âŒ Error writing to stream log: {e}")
    
    def finalize(self):
        """å®Œæˆæ—¥å¿—è®°å½•ï¼Œå†™å…¥æ±‡æ€»ä¿¡æ¯"""
        try:
            with open(self.log_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # æ·»åŠ æ±‡æ€»ä¿¡æ¯
            data.update({
                "end_time": datetime.now().isoformat(),
                "total_packets": self.packet_count,
                "total_size": sum(len(p.get("raw_data", "")) for p in self.packets),
                "packet_types": self._get_packet_type_stats(),
                "complete": True
            })
            
            with open(self.log_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… Stream log finalized: {self.log_file}")
            print(f"ğŸ“Š Total packets: {self.packet_count}, Types: {data['packet_types']}")
            
        except Exception as e:
            print(f"âŒ Error finalizing stream log: {e}")
    
    def _get_packet_type_stats(self) -> Dict[str, int]:
        """ç»Ÿè®¡ä¸åŒç±»å‹æ•°æ®åŒ…çš„æ•°é‡"""
        stats = {}
        for packet in self.packets:
            parsed = packet.get("parsed_data", {})
            
            # è¯†åˆ«æ•°æ®åŒ…ç±»å‹
            if "answer_piece" in parsed:
                packet_type = "answer_piece"
            elif "sub_question" in parsed:
                packet_type = "sub_question"
            elif "sub_query" in parsed:
                packet_type = "sub_query"
            elif "top_documents" in parsed:
                packet_type = "documents"
            elif "context_docs" in parsed:
                packet_type = "context_docs"
            elif "thinking_content" in parsed:
                packet_type = "thinking"
            elif "error" in parsed:
                packet_type = "error"
            elif "message" in parsed and "message_id" in parsed:
                packet_type = "message_detail"
            else:
                packet_type = "other"
            
            stats[packet_type] = stats.get(packet_type, 0) + 1
        
        return stats


def create_stream_logger_wrapper(original_generator: Generator[str, None, None], 
                                request_info: Dict[str, Any] = None) -> Generator[str, None, None]:
    """
    åŒ…è£…åŸå§‹æµç”Ÿæˆå™¨ï¼Œæ·»åŠ æ—¥å¿—è®°å½•åŠŸèƒ½
    
    Args:
        original_generator: åŸå§‹çš„æµç”Ÿæˆå™¨
        request_info: è¯·æ±‚ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        åŒ…è£…åçš„æµç”Ÿæˆå™¨
    """
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨æµæ—¥å¿—
    if not ENABLE_STREAM_LOGGING:
        # å¦‚æœæœªå¯ç”¨ï¼Œç›´æ¥è¿”å›åŸå§‹ç”Ÿæˆå™¨
        yield from original_generator
        return
    
    logger = StreamLogger(request_info)
    
    try:
        for packet in original_generator:
            # è®°å½•æ•°æ®åŒ…
            logger.log_packet(packet, "sse")
            
            # ç»§ç»­ä¼ é€’æ•°æ®åŒ…ç»™å®¢æˆ·ç«¯
            yield packet
            
    except Exception as e:
        # è®°å½•é”™è¯¯
        error_packet = json.dumps({"error": str(e), "error_type": "stream_wrapper_error"})
        logger.log_packet(error_packet, "error")
        
        # é‡æ–°æŠ›å‡ºå¼‚å¸¸
        raise
        
    finally:
        # å®Œæˆæ—¥å¿—è®°å½•
        logger.finalize()


# ä¾¿æ·å‡½æ•°ï¼šåˆ›å»ºè¯·æ±‚ä¿¡æ¯å­—å…¸
def create_request_info(chat_message_req=None, user=None, **kwargs) -> Dict[str, Any]:
    """åˆ›å»ºè¯·æ±‚ä¿¡æ¯å­—å…¸"""
    info = {
        "timestamp": datetime.now().isoformat(),
        **kwargs
    }
    
    if chat_message_req:
        info.update({
            "message": getattr(chat_message_req, "message", None),
            "chat_session_id": getattr(chat_message_req, "chat_session_id", None),
            "persona_id": getattr(chat_message_req, "persona_id", None),
            "use_agentic_search": getattr(chat_message_req, "use_agentic_search", None),
        })
    
    if user:
        info.update({
            "user_id": getattr(user, "id", None),
            "user_email": getattr(user, "email", None),
        })
    
    return info