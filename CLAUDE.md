1. å¦‚æœä½ è¦è¿è¡Œä»£ç ï¼Œæ€»æ˜¯åŸºäºé¡¹ç›®çš„.conda ç¯å¢ƒè¿è¡Œã€‚


# Onyx Project Guide

## Overview

Onyx (formerly Danswer) is an open-source Gen-AI + Enterprise Search platform that connects to company docs, apps, and people. It provides a feature-rich chat interface, supports 40+ connectors (Google Drive, Slack, Confluence, etc.), and can be deployed anywhere - from laptop to cloud.

## Project Structure

```
onyx/
â”œâ”€â”€ backend/                    # FastAPI backend services
â”‚   â”œâ”€â”€ onyx/                  # Core business logic
â”‚   â”œâ”€â”€ ee/                    # Enterprise edition features
â”‚   â”œâ”€â”€ model_server/          # ML model serving
â”‚   â””â”€â”€ tests/                 # Backend tests
â”œâ”€â”€ web/                       # Next.js frontend
â”‚   â”œâ”€â”€ src/                   # React components and pages
â”‚   â””â”€â”€ public/                # Static assets
â”œâ”€â”€ onyx-chat-standalone/      # Standalone chat interface
â”œâ”€â”€ deployment/                # Deployment configurations
â”‚   â”œâ”€â”€ docker_compose/        # Docker compose files
â”‚   â””â”€â”€ helm/                  # Kubernetes Helm charts
â””â”€â”€ examples/                  # Example implementations
```

## Common Commands

### Backend Development

```bash
# Navigate to backend directory
cd backend/

# Install Python dependencies
pip install -r requirements/default.txt
pip install -r requirements/dev.txt  # For development

# Run database migrations
alembic upgrade head

# Start API server (development)
cd backend && AUTH_TYPE=disabled /Users/zhuxiaofeng/Github/onyx/.conda/bin/python -m uvicorn onyx.main:app --reload --port 8080

# Start model server
cd backend && AUTH_TYPE=disabled /Users/zhuxiaofeng/Github/onyx/.conda/bin/python -m uvicorn model_server.main:app --reload --port 9000

# Run backend tests
pytest tests/

# Type checking
mypy onyx/

# Code formatting
ruff check --fix .
```

### Background Task Services (Celery)

```bash
# Navigate to backend directory
cd backend/

# Start all background tasks (recommended)
cd backend && python scripts/dev_run_background_jobs.py

# å¦‚æœå‡ºç° Broken pipe é”™è¯¯ï¼Œå…ˆæ¸…ç†æ—§è¿›ç¨‹ï¼špkill -f celery
```

### Frontend Development

```bash
# Navigate to web directory
cd web/

# Install dependencies
npm install

# Start development server
npm run dev

# Build for production
npm run build

# Run production server
npm start

# Lint code
npm run lint
npm run lint:fix-unused  # Fix unused imports

# Run tests
npm test
```

### Onyx Chat Standalone

```bash
# Navigate to standalone chat
cd onyx-chat-standalone/

# Install dependencies
npm install

# Start development server (recommended)
cd onyx-chat-standalone && npm run dev

# Build for production
npm run build

# Run production server
npm start

# Type checking
npm run type-check

# Lint code
npm run lint
```

### Docker Development

```bash
# Start all services (development)
docker compose -f deployment/docker_compose/docker-compose.dev.yml up

# Start production services
docker compose -f deployment/docker_compose/docker-compose.prod.yml up

# Start with GPU support
docker compose -f deployment/docker_compose/docker-compose.gpu-dev.yml up

# Stop all services
docker compose -f deployment/docker_compose/docker-compose.dev.yml down
```

## High-Level Architecture

### System Components

1. **Web Frontend (Next.js)**
   - Modern React-based UI with TypeScript
   - Real-time chat interface with streaming responses
   - Document management and search capabilities
   - Admin panel for connector and system configuration

2. **API Backend (FastAPI)**
   - RESTful API serving frontend requests
   - Authentication & authorization (OAuth2, OIDC, SAML)
   - Document processing and indexing orchestration
   - Chat session management

3. **Model Server**
   - Serves embedding models for document vectorization
   - Handles LLM inference requests
   - Supports multiple model providers (OpenAI, Anthropic, etc.)

4. **Search Engine (Vespa)**
   - Distributed search and ranking engine
   - Stores document embeddings and metadata
   - Handles hybrid search (vector + keyword)
   - Manages document permissions

5. **Task Queue (Celery + Redis)**
   - Asynchronous document indexing
   - Scheduled connector syncs
   - Background processing tasks

6. **Database (PostgreSQL)**
   - User management and authentication
   - Connector configurations
   - Chat history and sessions
   - System metadata

### Key Data Flows

#### Document Indexing Flow
```
Data Source â†’ Connector â†’ Content Parser â†’ Chunker â†’ Embedder â†’ Vespa Index
```

#### Search/Chat Flow
```
User Query â†’ Query Processing â†’ Permission Check â†’ Vespa Search â†’ 
Context Building â†’ LLM Prompt â†’ Streaming Response â†’ User
```

#### Authentication Flow
```
User Login â†’ OAuth/SAML Provider â†’ JWT Token â†’ API Authorization â†’ 
Resource Access
```

### Core Features

1. **Connectors System**
   - 40+ pre-built connectors (Slack, Google Drive, Confluence, etc.)
   - Incremental and full sync capabilities
   - Permission-aware document access
   - Custom connector framework

2. **Search Capabilities**
   - Semantic search using embeddings
   - Hybrid search combining vector and keyword matching
   - Real-time permission filtering
   - Search result re-ranking

3. **Chat System**
   - Context-aware conversations
   - Citation generation with source links
   - Streaming responses
   - Custom AI assistants/personas

4. **Security & Access Control**
   - Role-based access control (Admin, Curator, Basic)
   - Document-level permissions
   - SSO integration (OIDC, SAML, OAuth2)
   - API key management

5. **Enterprise Features (EE)**
   - Advanced analytics and usage reporting
   - Priority support
   - Enhanced security features
   - Multi-tenancy support

### Technology Stack

- **Backend**: Python, FastAPI, SQLAlchemy, Celery
- **Frontend**: TypeScript, React, Next.js, Tailwind CSS
- **Database**: PostgreSQL, Redis
- **Search**: Vespa
- **ML/AI**: Multiple LLM providers, Custom embedding models
- **Infrastructure**: Docker, Kubernetes, Nginx

### Environment Configuration

Key environment variables:
- `AUTH_TYPE`: Authentication method (disabled, basic, google_oauth, oidc, saml)
- `POSTGRES_*`: Database connection settings
- `VESPA_*`: Search engine configuration
- `REDIS_*`: Cache and queue settings
- `GEN_AI_*`: LLM configuration
- `WEB_DOMAIN`: Frontend URL for CORS

### Development Tips

1. **Adding a New Connector**
   - Implement in `backend/onyx/connectors/`
   - Follow the `LoadConnector` and `PollConnector` interfaces
   - Register in the connector factory

2. **Modifying the UI**
   - Frontend code in `web/src/`
   - Components use Tailwind CSS for styling
   - API calls go through `web/src/lib/`

3. **Database Changes**
   - Create Alembic migration: `alembic revision -m "description"`
   - Migrations in `backend/alembic/versions/`

4. **Testing**
   - Backend: pytest-based tests in `backend/tests/`
   - Frontend: Jest tests with `npm test`
   - Integration tests available for full system testing

### å·²ç¦ç”¨çš„ä»»åŠ¡å¤‡å¿˜
- `check_for_external_group_sync` å’Œ `check_for_doc_permissions_sync` ä»»åŠ¡å·²åœ¨ beat_schedule.py ä¸­æ³¨é‡Šç¦ç”¨ã€‚
- è¿™ä¸¤ä¸ªä»»åŠ¡åˆ†åˆ«è´Ÿè´£å¤–éƒ¨ç”¨æˆ·ç»„æƒé™åŒæ­¥å’Œæ–‡æ¡£çº§æƒé™åŒæ­¥ï¼Œå¦‚éœ€é‡æ–°å¯ç”¨æƒé™æ§åˆ¶åŠŸèƒ½éœ€è¦å–æ¶ˆæ³¨é‡Šã€‚

### è°ƒè¯•å·¥å…·å’Œé…ç½®
- **æµå¼æ—¥å¿—ç›‘æ§**: è®¾ç½®ç¯å¢ƒå˜é‡ `ENABLE_STREAM_LOGGING=true` å¯ç”¨ SSE æµå¼å“åº”çš„è¯¦ç»†æ—¥å¿—è®°å½•
- **æ—¥å¿—ä½ç½®**: é»˜è®¤å­˜å‚¨åœ¨ `/tmp/onyx_stream_logs/`ï¼Œå¯é€šè¿‡ `STREAM_LOG_DIR` ç¯å¢ƒå˜é‡è‡ªå®šä¹‰
- **ç›‘æ§å·¥å…·**: 
  - `python backend/scripts/start_stream_monitoring.py` - å®æ—¶ç›‘æ§æ–°çš„æµå¼æ—¥å¿—
  - `python backend/scripts/view_stream_logs.py --latest` - æŸ¥çœ‹æœ€æ–°çš„æµå¼æ—¥å¿—è¯¦æƒ…
  - `python backend/scripts/view_stream_logs.py --help` - æŸ¥çœ‹è¯¦ç»†ä½¿ç”¨è¯´æ˜
- **ç”¨é€”**: ç”¨äºè°ƒè¯•å­é—®é¢˜å¼•ç”¨ã€æ–‡æ¡£æœç´¢ã€æµå¼å“åº”ç­‰ SSE ç›¸å…³é—®é¢˜


å®Œæ•´è°ƒç”¨é“¾è·¯å›¾

  Frontend (onyx-chat-standalone)
  â”œâ”€â”€ useChat.sendMessage() [src/hooks/useChat.ts:178]
  â”œâ”€â”€ apiClient.sendMessage() [src/lib/api.ts:272]
  â”‚
  HTTP POST /chat/send-message
  â”‚
  Backend (FastAPI)
  â”œâ”€â”€ handle_new_chat_message() [chat_backend.py:403]
  â”œâ”€â”€ stream_chat_message() [process_message.py:1240]
  â”œâ”€â”€ stream_chat_message_objects() [process_message.py:528]
  â”‚   â”œâ”€â”€ get_chat_session_by_id()
  â”‚   â”œâ”€â”€ get_llms_for_persona() [factory.py:42]
  â”‚   â””â”€â”€ Answer() [answer.py:47]
  â”‚       â””â”€â”€ processed_streamed_output
  â”‚           â”œâ”€â”€ run_agent_search_graph() [æ™ºèƒ½æœç´¢æ¨¡å¼]
  â”‚           â”œâ”€â”€ run_basic_graph() [åŸºç¡€æ¨¡å¼]  
  â”‚           â”œâ”€â”€ run_kb_graph() [çŸ¥è¯†å›¾è°±æ¨¡å¼]
  â”‚           â””â”€â”€ run_dc_graph() [åˆ†æ²»æ¨¡å¼]
  â”‚
  LLM Layer
  â”œâ”€â”€ DefaultMultiLLM [chat_llm.py:270]
  â”œâ”€â”€ _stream_implementation() [chat_llm.py:526]
  â”œâ”€â”€ litellm.completion() [chat_llm.py:416]
  â”‚   â”œâ”€â”€ OpenAI API
  â”‚   â”œâ”€â”€ Anthropic API
  â”‚   â”œâ”€â”€ Local Models
  â”‚   â””â”€â”€ Other Providers
  â”‚
  Response Stream (SSE)
  â”œâ”€â”€ JSON packets (OnyxAnswerPiece, AgentAnswerPiece, etc.)
  â”œâ”€â”€ Document search results
  â”œâ”€â”€ Citations
  â””â”€â”€ Final message details

  å…³é”®æ‰§è¡Œç±»è¯¦è§£

  1. CreateChatMessageRequest: è¯·æ±‚å‚æ•°æ¨¡å‹
  2. Answer: æ ¸å¿ƒåè°ƒå™¨ï¼Œç®¡ç†æ•´ä¸ªå¯¹è¯æµç¨‹
  3. DefaultMultiLLM: LLM æŠ½è±¡å±‚ï¼Œç»Ÿä¸€å„ç§ LLM è°ƒç”¨
  4. StreamingProcessor (å‰ç«¯): å¤„ç†æµå¼å“åº”è§£æ
  5. GraphConfig: é…ç½®ä¸åŒçš„æ‰§è¡Œå›¾ç­–ç•¥

  åˆ†æ”¯é“¾è·¯

  - æ™ºèƒ½æœç´¢æ¨¡å¼: ä½¿ç”¨ agent æœç´¢å›¾è¿›è¡Œå¤šè½®æœç´¢å’Œæ¨ç†
  - åŸºç¡€æ¨¡å¼: ç›´æ¥ LLM å¯¹è¯
  - çŸ¥è¯†å›¾è°±æ¨¡å¼: ç»“åˆ KG çš„å¢å¼ºå¯¹è¯
  - æ–‡ä»¶ä¸Šä¼ å¤„ç†: é€šè¿‡ /chat/file ç«¯ç‚¹ä¸Šä¼ å¹¶ç´¢å¼•æ–‡ä»¶



  æ™ºèƒ½æœç´¢æ¨¡å¼ [run_agent_search_graph]
  â”‚
  â”œâ”€â”€ ğŸ“‹ prepare_tool_input [å‡†å¤‡å·¥å…·è¾“å…¥]
  â”‚   â””â”€â”€ å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼Œå‡†å¤‡å·¥å…·è°ƒç”¨å‚æ•°
  â”‚
  â”œâ”€â”€ ğŸ”§ choose_tool [é€‰æ‹©å·¥å…·]
  â”‚   â””â”€â”€ å†³å®šä½¿ç”¨å“ªä¸ªå·¥å…·æˆ–æ˜¯å¦è¿›å…¥æ™ºèƒ½æœç´¢
  â”‚
  â”œâ”€â”€ ğŸŒŸ route_initial_tool_choice [è·¯ç”±å†³ç­–] 
  â”‚   â”œâ”€â”€ Branch A: call_tool â†’ basic_use_tool_response â†’ logging_node [å¸¸è§„å·¥å…·è°ƒç”¨]
  â”‚   â”œâ”€â”€ Branch B: start_agent_search [è¿›å…¥æ™ºèƒ½æœç´¢ä¸»æµç¨‹] â­
  â”‚   â””â”€â”€ Branch C: logging_node [ç›´æ¥ç»“æŸ]
  â”‚
  â”œâ”€â”€ ğŸš€ start_agent_search [å¯åŠ¨æ™ºèƒ½æœç´¢]
  â”‚   â”œâ”€â”€ å¹¶è¡Œæ‰§è¡Œï¼š
  â”‚   â”‚   â”œâ”€â”€ generate_initial_answer_subgraph [ç”Ÿæˆåˆå§‹ç­”æ¡ˆå­å›¾] ğŸ“Š
  â”‚   â”‚   â””â”€â”€ extract_entity_term [æå–å®ä½“å’Œæœ¯è¯­] ğŸ·ï¸
  â”‚   â”‚
  â”‚   â””â”€â”€ â¬‡ï¸ ç­‰å¾…ä¸¤ä¸ªå¹¶è¡Œä»»åŠ¡å®Œæˆ
  â”‚
  â”œâ”€â”€ ğŸ” generate_initial_answer_subgraph [åˆå§‹ç­”æ¡ˆç”Ÿæˆå­å›¾]
  â”‚   â”œâ”€â”€ å¹¶è¡Œæ‰§è¡Œï¼š
  â”‚   â”‚   â”œâ”€â”€ retrieve_orig_question_docs_subgraph [æ£€ç´¢åŸé—®é¢˜æ–‡æ¡£]
  â”‚   â”‚   â””â”€â”€ generate_sub_answers_subgraph [ç”Ÿæˆå­ç­”æ¡ˆå­å›¾]
  â”‚   â”‚       â”œâ”€â”€ decompose_orig_question [åˆ†è§£åŸé—®é¢˜] ğŸ§©
  â”‚   â”‚       â”‚   â””â”€â”€ ä½¿ç”¨ fast_llm å°†åŸé—®é¢˜åˆ†è§£ä¸ºå­é—®é¢˜
  â”‚   â”‚       â”œâ”€â”€ answer_sub_question_subgraphs [å¹¶è¡Œå›ç­”å­é—®é¢˜] ğŸ”€
  â”‚   â”‚       â”‚   â”œâ”€â”€ ä¸ºæ¯ä¸ªå­é—®é¢˜å¹¶è¡Œæ‰§è¡Œæœç´¢
  â”‚   â”‚       â”‚   â”œâ”€â”€ è°ƒç”¨æœç´¢å·¥å…·è·å–ç›¸å…³æ–‡æ¡£
  â”‚   â”‚       â”‚   â””â”€â”€ ä½¿ç”¨ä¸» LLM ç”Ÿæˆå­ç­”æ¡ˆ
  â”‚   â”‚       â””â”€â”€ format_initial_sub_answers [æ ¼å¼åŒ–åˆå§‹å­ç­”æ¡ˆ] ğŸ“
  â”‚   â”‚
  â”‚   â”œâ”€â”€ generate_initial_answer [ç”Ÿæˆåˆå§‹ç­”æ¡ˆ] âœ¨
  â”‚   â”‚   â””â”€â”€ åŸºäºæ£€ç´¢æ–‡æ¡£å’Œå­ç­”æ¡ˆç”Ÿæˆç»¼åˆåˆå§‹ç­”æ¡ˆ
  â”‚   â””â”€â”€ validate_initial_answer [éªŒè¯åˆå§‹ç­”æ¡ˆ] âœ…
  â”‚
  â”œâ”€â”€ ğŸ¤” decide_refinement_need [å†³å®šæ˜¯å¦éœ€è¦ç²¾ç‚¼]
  â”‚   â””â”€â”€ continue_to_refined_answer_or_end [è·¯ç”±å†³ç­–]
  â”‚       â”œâ”€â”€ Branch 1: create_refined_sub_questions [éœ€è¦ç²¾ç‚¼] ğŸ”„
  â”‚       â””â”€â”€ Branch 2: logging_node [ä¸éœ€è¦ç²¾ç‚¼ï¼Œç›´æ¥ç»“æŸ] ğŸ
  â”‚
  â”œâ”€â”€ ğŸ“ create_refined_sub_questions [åˆ›å»ºç²¾ç‚¼å­é—®é¢˜]
  â”‚   â”œâ”€â”€ åŸºäºåˆå§‹ç­”æ¡ˆå’Œæå–çš„å®ä½“/æœ¯è¯­
  â”‚   â”œâ”€â”€ ç”Ÿæˆæ›´å…·ä½“ã€æ›´æ·±å…¥çš„å­é—®é¢˜
  â”‚   â””â”€â”€ parallelize_refined_sub_question_answering [å¹¶è¡ŒåŒ–ç²¾ç‚¼å­é—®é¢˜å›ç­”]
  â”‚
  â”œâ”€â”€ ğŸ”€ answer_refined_question_subgraphs [å›ç­”ç²¾ç‚¼å­é—®é¢˜å­å›¾]
  â”‚   â”œâ”€â”€ ä¸ºæ¯ä¸ªç²¾ç‚¼å­é—®é¢˜å¹¶è¡Œæ‰§è¡Œ
  â”‚   â”œâ”€â”€ æ›´æ·±å…¥çš„æœç´¢å’Œåˆ†æ
  â”‚   â””â”€â”€ ç”Ÿæˆé«˜è´¨é‡çš„å­ç­”æ¡ˆ
  â”‚
  â”œâ”€â”€ ğŸ“¥ ingest_refined_sub_answers [æ¶ˆåŒ–ç²¾ç‚¼å­ç­”æ¡ˆ]
  â”‚   â””â”€â”€ æ”¶é›†å’Œæ•´ç†æ‰€æœ‰ç²¾ç‚¼å­ç­”æ¡ˆ
  â”‚
  â”œâ”€â”€ âš¡ generate_validate_refined_answer [ç”Ÿæˆå¹¶éªŒè¯ç²¾ç‚¼ç­”æ¡ˆ]
  â”‚   â””â”€â”€ åŸºäºç²¾ç‚¼å­ç­”æ¡ˆç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
  â”‚
  â”œâ”€â”€ âš–ï¸ compare_answers [æ¯”è¾ƒç­”æ¡ˆ]
  â”‚   â”œâ”€â”€ æ¯”è¾ƒåˆå§‹ç­”æ¡ˆå’Œç²¾ç‚¼ç­”æ¡ˆ
  â”‚   â”œâ”€â”€ å†³å®šä½¿ç”¨å“ªä¸ªç­”æ¡ˆ
  â”‚   â””â”€â”€ è¯„ä¼°ç²¾ç‚¼æ˜¯å¦æœ‰æ”¹è¿›
  â”‚
  â””â”€â”€ ğŸ“Š logging_node [æ—¥å¿—è®°å½•] â†’ END
      â”œâ”€â”€ è®°å½•æ‰€æœ‰ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯
      â”œâ”€â”€ æŒä¹…åŒ–ç­”æ¡ˆã€å­é—®é¢˜å’Œå­ç­”æ¡ˆ
      â””â”€â”€ å®Œæˆæ•´ä¸ªæ™ºèƒ½æœç´¢æµç¨‹
